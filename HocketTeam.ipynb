{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c36b502-dde9-4d3a-8020-9377a921746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 11:32:58,106 - INFO - Subpages have been saved, zipped, and actual data extracted to Excel successfully with two sheets: 'NHL Stats 1990-2011' and 'Winner and Loser per Year'.\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import zipfile\n",
    "from urllib.parse import urljoin\n",
    "import xlsxwriter\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "async def fetch(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def get_subpages(base_url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session, base_url)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        subpages = set()  # Use a set to avoid duplicates\n",
    "        \n",
    "        # Find all links on the page in the order they appear\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/pages/forms/') and href != '/pages/forms/':\n",
    "                full_url = urljoin(base_url, href)\n",
    "                subpages.add(full_url)  # Add to set to ensure uniqueness\n",
    "        return list(subpages)\n",
    "\n",
    "async def save_subpages(subpages):\n",
    "    if not os.path.exists('HockeyTeams'):\n",
    "        os.makedirs('HockeyTeams')\n",
    "    \n",
    "    all_data = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for i, url in enumerate(subpages, 1):\n",
    "            tasks.append(fetch(session, url))\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for i, html in enumerate(responses, 1):\n",
    "            filename = f'HockeyTeams/{i}.html'\n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(html)\n",
    "\n",
    "            # Extract structured data from the HTML table rows\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            table = soup.find('table')\n",
    "            if table:\n",
    "                headers = [th.text.strip() for th in table.find_all('th')]\n",
    "                for row in table.find_all('tr')[1:]:  # Skip header row\n",
    "                    columns = row.find_all('td')\n",
    "                    if columns:\n",
    "                        row_data = {headers[i]: columns[i].text.strip() for i in range(len(columns))}\n",
    "                        all_data.append(row_data)\n",
    "    \n",
    "    # Remove duplicates from all_data\n",
    "    unique_data = [dict(t) for t in {tuple(sorted(row.items())) for row in all_data}]\n",
    "    return headers, unique_data\n",
    "\n",
    "def create_zip():\n",
    "    with zipfile.ZipFile('HockeyTeams.zip', 'w') as zipf:\n",
    "        for root, dirs, files in os.walk('HockeyTeams'):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), 'HockeyTeams'))\n",
    "\n",
    "def generate_winner_loser_summary(all_data):\n",
    "    summary = {}\n",
    "    for row in all_data:\n",
    "        year = row.get('Year')\n",
    "        team = row.get('Team Name')\n",
    "        wins = row.get('Wins', '0').strip()\n",
    "        if year and team and wins.isdigit():\n",
    "            wins = int(wins)\n",
    "            if year not in summary:\n",
    "                summary[year] = {}\n",
    "            summary[year][team] = wins\n",
    "    \n",
    "    winner_loser_data = []\n",
    "    for year, teams in sorted(summary.items()):  # Sort by year\n",
    "        if teams:  # Ensure there are valid teams\n",
    "            winner = max(teams.items(), key=lambda x: x[1])  # Team with most wins\n",
    "            loser = min(teams.items(), key=lambda x: x[1])   # Team with least wins\n",
    "            winner_loser_data.append([\n",
    "                year, winner[0], winner[1], loser[0], loser[1]\n",
    "            ])\n",
    "        else:\n",
    "            logging.warning(f\"No valid data for year {year}\")\n",
    "    return winner_loser_data\n",
    "\n",
    "def save_to_excel(headers, all_data):\n",
    "    workbook = xlsxwriter.Workbook('HockeyTeams_data.xlsx')\n",
    "    \n",
    "    # Sheet 1: NHL Stats 1990-2011\n",
    "    worksheet1 = workbook.add_worksheet('NHL Stats 1990-2011')\n",
    "    for col_num, header in enumerate(headers):\n",
    "        worksheet1.write(0, col_num, header)\n",
    "    for row_num, row_data in enumerate(all_data, start=1):\n",
    "        for col_num, header in enumerate(headers):\n",
    "            worksheet1.write(row_num, col_num, row_data.get(header, ''))\n",
    "    \n",
    "    # Sheet 2: Winner and Loser per Year\n",
    "    worksheet2 = workbook.add_worksheet('Winner and Loser per Year')\n",
    "    summary_headers = ['Year', 'Winner', 'Winner Num. of Wins', 'Loser', 'Loser Num. of Wins']\n",
    "    for col_num, header in enumerate(summary_headers):\n",
    "        worksheet2.write(0, col_num, header)\n",
    "    \n",
    "    summary_data = generate_winner_loser_summary(all_data)\n",
    "    for row_num, summary_row in enumerate(summary_data, start=1):\n",
    "        for col_num, cell_data in enumerate(summary_row):\n",
    "            worksheet2.write(row_num, col_num, cell_data)\n",
    "    \n",
    "    workbook.close()\n",
    "\n",
    "async def main():\n",
    "    base_url = 'https://www.scrapethissite.com/pages/forms/'\n",
    "    subpages = await get_subpages(base_url)\n",
    "    headers, all_data = await save_subpages(subpages)\n",
    "    create_zip()\n",
    "    save_to_excel(headers, all_data)\n",
    "    logging.info(\"Subpages have been saved, zipped, and actual data extracted to Excel successfully with two sheets: 'NHL Stats 1990-2011' and 'Winner and Loser per Year'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46dcbbd-3576-44af-b9bb-cb5a1f6ee200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
